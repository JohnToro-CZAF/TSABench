{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from adapters import AutoAdapterModel\n",
    "\n",
    "from transformers import BertConfig\n",
    "from transformers import BertTokenizer\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login(key=\"d1209ac0e4a568567e407d3afef99099ea87a4b1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_PROJECT=peft\n",
      "env: WANDB_ENTITY=SC4001\n"
     ]
    }
   ],
   "source": [
    "%env WANDB_PROJECT=peft\n",
    "%env WANDB_ENTITY=SC4001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path, databilder:  rotten_tomatoes\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train': 8530, 'validation': 1066, 'test': 1066}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"rotten_tomatoes\")\n",
    "dataset.num_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb6b1436c1f74705b31233a410eab5d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8530 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e96673e6f84d4700a8213168e75f072e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1066 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25c5eb576257494ea8899d9cdfae315c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1066 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def encode_batch(batch):\n",
    "    \"\"\"Encodes a batch of input data using the model tokenizer.\"\"\"\n",
    "    return tokenizer(batch[\"text\"], max_length=80, truncation=True, padding=\"max_length\")\n",
    "\n",
    "dataset = dataset.map(encode_batch, batched=True)\n",
    "dataset = dataset.rename_column(original_column_name=\"label\", new_column_name=\"labels\")\n",
    "dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from adapters import IA3Config, SeqBnConfig, LoRAConfig, ReftConfig, CompacterConfig\n",
    "\n",
    "ia3_config = IA3Config()\n",
    "seq_bn_config = SeqBnConfig()\n",
    "lora_config = LoRAConfig()\n",
    "compacter_config = CompacterConfig()\n",
    "reft_config = ReftConfig(layers=\"all\", prefix_positions=3, suffix_positions=0, r=1, orthogonality=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertAdapterModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['heads.default.3.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "config = BertConfig.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    num_labels=2,\n",
    ")\n",
    "model = AutoAdapterModel.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "An adapter with the name 'rotten_tomatoes' has already been added.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[83], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m runname \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbert-rotten_tomatoes-lora\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_adapter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrotten_tomatoes\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlora_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39madd_classification_head(\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrotten_tomatoes\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m     num_labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m      6\u001b[0m     id2label\u001b[38;5;241m=\u001b[39m{ \u001b[38;5;241m0\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müëé\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müëç\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Activate the adapter\u001b[39;00m\n",
      "File \u001b[0;32m/datadrive5/huypn16/anaconda3/envs/ana/lib/python3.11/site-packages/adapters/model_mixin.py:1536\u001b[0m, in \u001b[0;36mModelWithHeadsAdaptersMixin.add_adapter\u001b[0;34m(self, adapter_name, config, overwrite_ok, set_active)\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39madd_adapter(adapter_name, config, overwrite_ok\u001b[38;5;241m=\u001b[39moverwrite_ok, set_active\u001b[38;5;241m=\u001b[39mset_active)\n\u001b[1;32m   1535\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1536\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_adapter\u001b[49m\u001b[43m(\u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverwrite_ok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverwrite_ok\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mset_active\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mset_active\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/datadrive5/huypn16/anaconda3/envs/ana/lib/python3.11/site-packages/adapters/model_mixin.py:585\u001b[0m, in \u001b[0;36mModelAdaptersMixin.add_adapter\u001b[0;34m(self, adapter_name, config, overwrite_ok, set_active)\u001b[0m\n\u001b[1;32m    583\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m overwrite_ok \u001b[38;5;129;01mand\u001b[39;00m adapter_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madapters_config:\n\u001b[1;32m    584\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete_adapter(adapter_name)\n\u001b[0;32m--> 585\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madapters_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    586\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    587\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_adapter_weights(adapter_name)\n",
      "File \u001b[0;32m/datadrive5/huypn16/anaconda3/envs/ana/lib/python3.11/site-packages/adapters/configuration/model_adapters_config.py:118\u001b[0m, in \u001b[0;36mModelAdaptersConfig.add\u001b[0;34m(self, adapter_name, config)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;124;03mAdds a new adapter of the name to the model config.\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;124;03m    config (Optional[Union[str, dict]], optional): The adapter config. Defaults to None.\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m adapter_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madapters:\n\u001b[0;32m--> 118\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn adapter with the name \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00madapter_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m has already been added.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    120\u001b[0m     config \u001b[38;5;241m=\u001b[39m DEFAULT_ADAPTER_CONFIG\n",
      "\u001b[0;31mValueError\u001b[0m: An adapter with the name 'rotten_tomatoes' has already been added."
     ]
    }
   ],
   "source": [
    "runname = \"bert-rotten_tomatoes-lora\"\n",
    "model.add_adapter(\"rotten_tomatoes\", config=lora_config)\n",
    "model.add_classification_head(\n",
    "    \"rotten_tomatoes\",\n",
    "    num_labels=2,\n",
    "    id2label={ 0: \"üëé\", 1: \"üëç\"}\n",
    ")\n",
    "\n",
    "# Activate the adapter\n",
    "model.train_adapter(\"rotten_tomatoes\")\n",
    "\n",
    "# calculate the number of training parameters\n",
    "total = 0\n",
    "for p in model.parameters():\n",
    "    if p.requires_grad:\n",
    "        total += p.numel()\n",
    "print(f\"Total number of trainable parameters: {total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from transformers import TrainingArguments, EvalPrediction\n",
    "from adapters import AdapterTrainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    learning_rate=1e-4,\n",
    "    num_train_epochs=6,\n",
    "    per_device_train_batch_size=128,\n",
    "    per_device_eval_batch_size=128,\n",
    "    logging_steps=200,\n",
    "    output_dir=\"./training_output\",\n",
    "    overwrite_output_dir=True,\n",
    "    # The next line is important to ensure the dataset labels are properly passed to the model\n",
    "    remove_unused_columns=False,\n",
    "    run_name=runname\n",
    ")\n",
    "\n",
    "def compute_accuracy(p: EvalPrediction):\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    return {\"acc\": (preds == p.label_ids).mean()}\n",
    "\n",
    "trainer = AdapterTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"validation\"],\n",
    "    compute_metrics=compute_accuracy,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/datadrive5/huypn16/anaconda3/envs/ana/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='204' max='204' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [204/204 05:02, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.406600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: ERROR Error while calling W&B API: Error 3988 (HY000): Conversion from collation utf8mb4_0900_ai_ci into utf8mb3_general_ci impossible for parameter (<Response [422]>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=204, training_loss=0.407117425811057, metrics={'train_runtime': 306.3164, 'train_samples_per_second': 167.082, 'train_steps_per_second': 0.666, 'total_flos': 2135229632755200.0, 'train_loss': 0.407117425811057, 'epoch': 6.0})"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/datadrive5/huypn16/anaconda3/envs/ana/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.37011364102363586,\n",
       " 'eval_acc': 0.8311444652908068,\n",
       " 'eval_runtime': 3.1652,\n",
       " 'eval_samples_per_second': 336.785,\n",
       " 'eval_steps_per_second': 1.58,\n",
       " 'epoch': 6.0}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: ERROR Error while calling W&B API: Error 3988 (HY000): Conversion from collation utf8mb4_0900_ai_ci into utf8mb3_general_ci impossible for parameter (<Response [422]>)\n"
     ]
    }
   ],
   "source": [
    "trainer.evaluate(dataset[\"validation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
